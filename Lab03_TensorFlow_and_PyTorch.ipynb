{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs PyTorch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23ebc05e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ebc05e",
        "outputId": "3e8c1c94-b347-4e9c-e338-20f5ae6911ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3004 - accuracy: 0.9144\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1476 - accuracy: 0.9575\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1067 - accuracy: 0.9684\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0856 - accuracy: 0.9740\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0698 - accuracy: 0.9789\n",
            "TF Training time: 11.37 seconds\n",
            "313/313 [==============================] - 0s 894us/step - loss: 0.0901 - accuracy: 0.9721\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.09011811763048172, 0.972100019454956]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Number of hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Number of output neurons\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # Name of the loss function\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")       # Outputs the training time\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be6ab50a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be6ab50a",
        "outputId": "a07fa14c-b501-4f54-e06f-92613c960906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\vinit\\AppData\\Local\\Temp\\tmp9u1hml9a\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\vinit\\AppData\\Local\\Temp\\tmp9u1hml9a\\assets\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a69f4d2a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-win_amd64.whl (1.7 MB)\n",
            "     ---------------------------------------- 1.7/1.7 MB 15.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.24.3)\n",
            "Requirement already satisfied: torch==2.7.1 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.7.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (10.0.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.7.1->torchvision) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.7.1->torchvision) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.7.1->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.7.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.7.1->torchvision) (2025.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.7.1->torchvision) (2.1.3)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.22.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "623dfb49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "623dfb49",
        "outputId": "d60d6181-8687-41fa-ab2c-735815ea56d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 8.81MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 250kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.52MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Training time: 27.12 seconds\n",
            "Test accuracy: 0.9590\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)    # Input and output size\n",
        "        self.fc2 = nn.Linear(64, 10)    # Input and output size\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "WuMKMhHc8aLF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuMKMhHc8aLF",
        "outputId": "b49a73a4-7562-4742-bafe-c065014d4a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Using cached onnx-1.18.0-cp311-cp311-win_amd64.whl (15.8 MB)\n",
            "Requirement already satisfied: numpy>=1.22 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnx) (1.24.3)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnx) (6.31.1)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\users\\vinit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnx) (4.14.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Installing ONNX\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sv4P-dSS_GQB",
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "KH-sDlHq_Gdw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH-sDlHq_Gdw",
        "outputId": "328b31e1-79a8-44c6-9fb5-e4cb9fd2c2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3764, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.4498, Accuracy: 0.7206\n",
            "Step 200, Loss: 0.3370, Accuracy: 0.7987\n",
            "Step 300, Loss: 0.4641, Accuracy: 0.8287\n",
            "Step 400, Loss: 0.2334, Accuracy: 0.8466\n",
            "Step 500, Loss: 0.3494, Accuracy: 0.8602\n",
            "Step 600, Loss: 0.1779, Accuracy: 0.8703\n",
            "Step 700, Loss: 0.2754, Accuracy: 0.8764\n",
            "Step 800, Loss: 0.2889, Accuracy: 0.8821\n",
            "Step 900, Loss: 0.4438, Accuracy: 0.8867\n",
            "Step 1000, Loss: 0.0507, Accuracy: 0.8919\n",
            "Step 1100, Loss: 0.1385, Accuracy: 0.8949\n",
            "Step 1200, Loss: 0.1681, Accuracy: 0.8985\n",
            "Step 1300, Loss: 0.1318, Accuracy: 0.9017\n",
            "Step 1400, Loss: 0.2969, Accuracy: 0.9038\n",
            "Step 1500, Loss: 0.2227, Accuracy: 0.9059\n",
            "Step 1600, Loss: 0.1714, Accuracy: 0.9086\n",
            "Step 1700, Loss: 0.0311, Accuracy: 0.9113\n",
            "Step 1800, Loss: 0.3433, Accuracy: 0.9132\n",
            "Training Accuracy for epoch 1: 0.9147\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0423, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0710, Accuracy: 0.9579\n",
            "Step 200, Loss: 0.0516, Accuracy: 0.9512\n",
            "Step 300, Loss: 0.2131, Accuracy: 0.9530\n",
            "Step 400, Loss: 0.0357, Accuracy: 0.9535\n",
            "Step 500, Loss: 0.0658, Accuracy: 0.9546\n",
            "Step 600, Loss: 0.2399, Accuracy: 0.9547\n",
            "Step 700, Loss: 0.0362, Accuracy: 0.9546\n",
            "Step 800, Loss: 0.0505, Accuracy: 0.9547\n",
            "Step 900, Loss: 0.0347, Accuracy: 0.9554\n",
            "Step 1000, Loss: 0.2556, Accuracy: 0.9556\n",
            "Step 1100, Loss: 0.0735, Accuracy: 0.9554\n",
            "Step 1200, Loss: 0.2231, Accuracy: 0.9555\n",
            "Step 1300, Loss: 0.0810, Accuracy: 0.9559\n",
            "Step 1400, Loss: 0.1523, Accuracy: 0.9565\n",
            "Step 1500, Loss: 0.0874, Accuracy: 0.9563\n",
            "Step 1600, Loss: 0.0396, Accuracy: 0.9572\n",
            "Step 1700, Loss: 0.0549, Accuracy: 0.9580\n",
            "Step 1800, Loss: 0.0435, Accuracy: 0.9585\n",
            "Training Accuracy for epoch 2: 0.9589\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1397, Accuracy: 0.9062\n",
            "Step 100, Loss: 0.1858, Accuracy: 0.9623\n",
            "Step 200, Loss: 0.2105, Accuracy: 0.9644\n",
            "Step 300, Loss: 0.0484, Accuracy: 0.9661\n",
            "Step 400, Loss: 0.0426, Accuracy: 0.9668\n",
            "Step 500, Loss: 0.0074, Accuracy: 0.9669\n",
            "Step 600, Loss: 0.0869, Accuracy: 0.9668\n",
            "Step 700, Loss: 0.0166, Accuracy: 0.9674\n",
            "Step 800, Loss: 0.0342, Accuracy: 0.9673\n",
            "Step 900, Loss: 0.0751, Accuracy: 0.9677\n",
            "Step 1000, Loss: 0.1652, Accuracy: 0.9675\n",
            "Step 1100, Loss: 0.1218, Accuracy: 0.9677\n",
            "Step 1200, Loss: 0.0599, Accuracy: 0.9675\n",
            "Step 1300, Loss: 0.0580, Accuracy: 0.9675\n",
            "Step 1400, Loss: 0.0307, Accuracy: 0.9677\n",
            "Step 1500, Loss: 0.0614, Accuracy: 0.9678\n",
            "Step 1600, Loss: 0.0564, Accuracy: 0.9680\n",
            "Step 1700, Loss: 0.0085, Accuracy: 0.9683\n",
            "Step 1800, Loss: 0.0356, Accuracy: 0.9687\n",
            "Training Accuracy for epoch 3: 0.9689\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0737, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0505, Accuracy: 0.9725\n",
            "Step 200, Loss: 0.1290, Accuracy: 0.9742\n",
            "Step 300, Loss: 0.3077, Accuracy: 0.9749\n",
            "Step 400, Loss: 0.0158, Accuracy: 0.9761\n",
            "Step 500, Loss: 0.0310, Accuracy: 0.9760\n",
            "Step 600, Loss: 0.0590, Accuracy: 0.9757\n",
            "Step 700, Loss: 0.1369, Accuracy: 0.9758\n",
            "Step 800, Loss: 0.1225, Accuracy: 0.9756\n",
            "Step 900, Loss: 0.0457, Accuracy: 0.9755\n",
            "Step 1000, Loss: 0.0543, Accuracy: 0.9751\n",
            "Step 1100, Loss: 0.1033, Accuracy: 0.9752\n",
            "Step 1200, Loss: 0.2105, Accuracy: 0.9752\n",
            "Step 1300, Loss: 0.0254, Accuracy: 0.9753\n",
            "Step 1400, Loss: 0.0145, Accuracy: 0.9752\n",
            "Step 1500, Loss: 0.0384, Accuracy: 0.9756\n",
            "Step 1600, Loss: 0.0490, Accuracy: 0.9758\n",
            "Step 1700, Loss: 0.0022, Accuracy: 0.9757\n",
            "Step 1800, Loss: 0.0505, Accuracy: 0.9758\n",
            "Training Accuracy for epoch 4: 0.9754\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0188, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0134, Accuracy: 0.9811\n",
            "Step 200, Loss: 0.1044, Accuracy: 0.9793\n",
            "Step 300, Loss: 0.0580, Accuracy: 0.9789\n",
            "Step 400, Loss: 0.0522, Accuracy: 0.9783\n",
            "Step 500, Loss: 0.0609, Accuracy: 0.9790\n",
            "Step 600, Loss: 0.0140, Accuracy: 0.9788\n",
            "Step 700, Loss: 0.0108, Accuracy: 0.9789\n",
            "Step 800, Loss: 0.0478, Accuracy: 0.9791\n",
            "Step 900, Loss: 0.0715, Accuracy: 0.9792\n",
            "Step 1000, Loss: 0.0206, Accuracy: 0.9786\n",
            "Step 1100, Loss: 0.0162, Accuracy: 0.9790\n",
            "Step 1200, Loss: 0.1999, Accuracy: 0.9789\n",
            "Step 1300, Loss: 0.0275, Accuracy: 0.9789\n",
            "Step 1400, Loss: 0.0204, Accuracy: 0.9787\n",
            "Step 1500, Loss: 0.1564, Accuracy: 0.9789\n",
            "Step 1600, Loss: 0.1715, Accuracy: 0.9789\n",
            "Step 1700, Loss: 0.0414, Accuracy: 0.9789\n",
            "Step 1800, Loss: 0.0608, Accuracy: 0.9791\n",
            "Training Accuracy for epoch 5: 0.9790\n",
            "\n",
            "TF Training time: 103.06 seconds\n",
            "Test Accuracy: 0.9691\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalization factor\n",
        "x_test = x_test / 255.0   # Normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Batch size as in first TF example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Defining model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Number of neurons and activation\n",
        "])\n",
        "\n",
        "# Defining loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E4Nlg4lb_qdW",
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "source": [
        "## Performance Optimization with Graph Execution using @tf.function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Jmu_hciK_qle",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmu_hciK_qle",
        "outputId": "e3a9b487-4b64-48ef-e2a4-e45f59ccd703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3757, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.6098, Accuracy: 0.7277\n",
            "Step 200, Loss: 0.5080, Accuracy: 0.8033\n",
            "Step 300, Loss: 0.3916, Accuracy: 0.8341\n",
            "Step 400, Loss: 0.6318, Accuracy: 0.8519\n",
            "Step 500, Loss: 0.2983, Accuracy: 0.8646\n",
            "Step 600, Loss: 0.3261, Accuracy: 0.8716\n",
            "Step 700, Loss: 0.2038, Accuracy: 0.8787\n",
            "Step 800, Loss: 0.2351, Accuracy: 0.8837\n",
            "Step 900, Loss: 0.1710, Accuracy: 0.8895\n",
            "Step 1000, Loss: 0.5060, Accuracy: 0.8937\n",
            "Step 1100, Loss: 0.0736, Accuracy: 0.8976\n",
            "Step 1200, Loss: 0.3479, Accuracy: 0.9006\n",
            "Step 1300, Loss: 0.1611, Accuracy: 0.9034\n",
            "Step 1400, Loss: 0.1343, Accuracy: 0.9057\n",
            "Step 1500, Loss: 0.0496, Accuracy: 0.9086\n",
            "Step 1600, Loss: 0.0645, Accuracy: 0.9109\n",
            "Step 1700, Loss: 0.0671, Accuracy: 0.9131\n",
            "Step 1800, Loss: 0.2054, Accuracy: 0.9153\n",
            "Training Accuracy for epoch 1: 0.9169\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1082, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.2367, Accuracy: 0.9471\n",
            "Step 200, Loss: 0.0786, Accuracy: 0.9509\n",
            "Step 300, Loss: 0.0875, Accuracy: 0.9523\n",
            "Step 400, Loss: 0.2492, Accuracy: 0.9538\n",
            "Step 500, Loss: 0.1775, Accuracy: 0.9540\n",
            "Step 600, Loss: 0.0259, Accuracy: 0.9548\n",
            "Step 700, Loss: 0.2011, Accuracy: 0.9547\n",
            "Step 800, Loss: 0.1226, Accuracy: 0.9553\n",
            "Step 900, Loss: 0.0637, Accuracy: 0.9556\n",
            "Step 1000, Loss: 0.1530, Accuracy: 0.9562\n",
            "Step 1100, Loss: 0.1566, Accuracy: 0.9561\n",
            "Step 1200, Loss: 0.0780, Accuracy: 0.9561\n",
            "Step 1300, Loss: 0.0681, Accuracy: 0.9564\n",
            "Step 1400, Loss: 0.0619, Accuracy: 0.9564\n",
            "Step 1500, Loss: 0.0983, Accuracy: 0.9569\n",
            "Step 1600, Loss: 0.0221, Accuracy: 0.9573\n",
            "Step 1700, Loss: 0.0829, Accuracy: 0.9579\n",
            "Step 1800, Loss: 0.2309, Accuracy: 0.9580\n",
            "Training Accuracy for epoch 2: 0.9583\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0205, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.1011, Accuracy: 0.9675\n",
            "Step 200, Loss: 0.0780, Accuracy: 0.9689\n",
            "Step 300, Loss: 0.1592, Accuracy: 0.9699\n",
            "Step 400, Loss: 0.0996, Accuracy: 0.9691\n",
            "Step 500, Loss: 0.0501, Accuracy: 0.9687\n",
            "Step 600, Loss: 0.0277, Accuracy: 0.9690\n",
            "Step 700, Loss: 0.3270, Accuracy: 0.9686\n",
            "Step 800, Loss: 0.0950, Accuracy: 0.9691\n",
            "Step 900, Loss: 0.1035, Accuracy: 0.9694\n",
            "Step 1000, Loss: 0.0242, Accuracy: 0.9698\n",
            "Step 1100, Loss: 0.0558, Accuracy: 0.9695\n",
            "Step 1200, Loss: 0.0603, Accuracy: 0.9697\n",
            "Step 1300, Loss: 0.0097, Accuracy: 0.9699\n",
            "Step 1400, Loss: 0.0550, Accuracy: 0.9696\n",
            "Step 1500, Loss: 0.2047, Accuracy: 0.9697\n",
            "Step 1600, Loss: 0.1485, Accuracy: 0.9697\n",
            "Step 1700, Loss: 0.0829, Accuracy: 0.9696\n",
            "Step 1800, Loss: 0.0317, Accuracy: 0.9698\n",
            "Training Accuracy for epoch 3: 0.9699\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.1965, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0520, Accuracy: 0.9780\n",
            "Step 200, Loss: 0.0213, Accuracy: 0.9778\n",
            "Step 300, Loss: 0.0519, Accuracy: 0.9783\n",
            "Step 400, Loss: 0.0295, Accuracy: 0.9783\n",
            "Step 500, Loss: 0.0749, Accuracy: 0.9763\n",
            "Step 600, Loss: 0.1318, Accuracy: 0.9756\n",
            "Step 700, Loss: 0.0476, Accuracy: 0.9750\n",
            "Step 800, Loss: 0.1228, Accuracy: 0.9751\n",
            "Step 900, Loss: 0.0636, Accuracy: 0.9751\n",
            "Step 1000, Loss: 0.1807, Accuracy: 0.9754\n",
            "Step 1100, Loss: 0.0416, Accuracy: 0.9751\n",
            "Step 1200, Loss: 0.1787, Accuracy: 0.9755\n",
            "Step 1300, Loss: 0.0459, Accuracy: 0.9755\n",
            "Step 1400, Loss: 0.1003, Accuracy: 0.9757\n",
            "Step 1500, Loss: 0.0977, Accuracy: 0.9758\n",
            "Step 1600, Loss: 0.0473, Accuracy: 0.9759\n",
            "Step 1700, Loss: 0.0111, Accuracy: 0.9762\n",
            "Step 1800, Loss: 0.1044, Accuracy: 0.9763\n",
            "Training Accuracy for epoch 4: 0.9762\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0059, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.1540, Accuracy: 0.9821\n",
            "Step 200, Loss: 0.0650, Accuracy: 0.9817\n",
            "Step 300, Loss: 0.0284, Accuracy: 0.9802\n",
            "Step 400, Loss: 0.0241, Accuracy: 0.9799\n",
            "Step 500, Loss: 0.0113, Accuracy: 0.9799\n",
            "Step 600, Loss: 0.0597, Accuracy: 0.9800\n",
            "Step 700, Loss: 0.0449, Accuracy: 0.9800\n",
            "Step 800, Loss: 0.1002, Accuracy: 0.9797\n",
            "Step 900, Loss: 0.0750, Accuracy: 0.9795\n",
            "Step 1000, Loss: 0.0792, Accuracy: 0.9795\n",
            "Step 1100, Loss: 0.0151, Accuracy: 0.9796\n",
            "Step 1200, Loss: 0.0986, Accuracy: 0.9794\n",
            "Step 1300, Loss: 0.0262, Accuracy: 0.9794\n",
            "Step 1400, Loss: 0.0525, Accuracy: 0.9796\n",
            "Step 1500, Loss: 0.0463, Accuracy: 0.9793\n",
            "Step 1600, Loss: 0.1783, Accuracy: 0.9794\n",
            "Step 1700, Loss: 0.0318, Accuracy: 0.9795\n",
            "Step 1800, Loss: 0.1677, Accuracy: 0.9795\n",
            "Training Accuracy for epoch 5: 0.9796\n",
            "\n",
            "TF Training time: 8.92 seconds\n",
            "Test Accuracy: 0.9723\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalization factor\n",
        "x_test = x_test / 255.0   # Normalization factor\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Defining model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    #  Number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Number of neurons and activation\n",
        "])\n",
        "\n",
        "# Defining loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compiling the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
