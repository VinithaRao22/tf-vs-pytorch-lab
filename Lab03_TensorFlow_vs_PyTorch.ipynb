{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow and PyTorch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ebc05e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ebc05e",
        "outputId": "3e8c1c94-b347-4e9c-e338-20f5ae6911ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8602 - loss: 0.5057\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9560 - loss: 0.1529\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9685 - loss: 0.1084\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9745 - loss: 0.0857\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9799 - loss: 0.0679\n",
            "TF Training time: 15.76 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9675 - loss: 0.1030  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.09240816533565521, 0.9714000225067139]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Number of hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Number of output neurons\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # Name of the loss function\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")       # Outputs the training time\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6ab50a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be6ab50a",
        "outputId": "a07fa14c-b501-4f54-e06f-92613c960906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/tmp/tmplh2opi__'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  138628117417424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138628117420112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138628117420304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138628117420496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "623dfb49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "623dfb49",
        "outputId": "d60d6181-8687-41fa-ab2c-735815ea56d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 495kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.61MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Training time: 37.03 seconds\n",
            "Test accuracy: 0.9682\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)    # Input and output size\n",
        "        self.fc2 = nn.Linear(64, 10)    # Input and output size\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WuMKMhHc8aLF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuMKMhHc8aLF",
        "outputId": "b49a73a4-7562-4742-bafe-c065014d4a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ],
      "source": [
        "# Installing ONNX\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sv4P-dSS_GQB",
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KH-sDlHq_Gdw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH-sDlHq_Gdw",
        "outputId": "328b31e1-79a8-44c6-9fb5-e4cb9fd2c2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3997, Accuracy: 0.0312\n",
            "Step 100, Loss: 0.4086, Accuracy: 0.7222\n",
            "Step 200, Loss: 0.2637, Accuracy: 0.7965\n",
            "Step 300, Loss: 0.2558, Accuracy: 0.8258\n",
            "Step 400, Loss: 0.5424, Accuracy: 0.8448\n",
            "Step 500, Loss: 0.3789, Accuracy: 0.8582\n",
            "Step 600, Loss: 0.4689, Accuracy: 0.8681\n",
            "Step 700, Loss: 0.4306, Accuracy: 0.8761\n",
            "Step 800, Loss: 0.1280, Accuracy: 0.8826\n",
            "Step 900, Loss: 0.1377, Accuracy: 0.8876\n",
            "Step 1000, Loss: 0.2222, Accuracy: 0.8921\n",
            "Step 1100, Loss: 0.3313, Accuracy: 0.8961\n",
            "Step 1200, Loss: 0.3341, Accuracy: 0.8988\n",
            "Step 1300, Loss: 0.1639, Accuracy: 0.9018\n",
            "Step 1400, Loss: 0.2513, Accuracy: 0.9042\n",
            "Step 1500, Loss: 0.1224, Accuracy: 0.9071\n",
            "Step 1600, Loss: 0.0844, Accuracy: 0.9094\n",
            "Step 1700, Loss: 0.1984, Accuracy: 0.9120\n",
            "Step 1800, Loss: 0.2663, Accuracy: 0.9138\n",
            "Training Accuracy for epoch 1: 0.9154\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.3654, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.1198, Accuracy: 0.9573\n",
            "Step 200, Loss: 0.0320, Accuracy: 0.9558\n",
            "Step 300, Loss: 0.2453, Accuracy: 0.9562\n",
            "Step 400, Loss: 0.1664, Accuracy: 0.9553\n",
            "Step 500, Loss: 0.2401, Accuracy: 0.9559\n",
            "Step 600, Loss: 0.0580, Accuracy: 0.9561\n",
            "Step 700, Loss: 0.1237, Accuracy: 0.9557\n",
            "Step 800, Loss: 0.1657, Accuracy: 0.9563\n",
            "Step 900, Loss: 0.1504, Accuracy: 0.9562\n",
            "Step 1000, Loss: 0.0281, Accuracy: 0.9563\n",
            "Step 1100, Loss: 0.1118, Accuracy: 0.9567\n",
            "Step 1200, Loss: 0.0571, Accuracy: 0.9572\n",
            "Step 1300, Loss: 0.0772, Accuracy: 0.9575\n",
            "Step 1400, Loss: 0.1891, Accuracy: 0.9571\n",
            "Step 1500, Loss: 0.0940, Accuracy: 0.9573\n",
            "Step 1600, Loss: 0.1085, Accuracy: 0.9575\n",
            "Step 1700, Loss: 0.0217, Accuracy: 0.9577\n",
            "Step 1800, Loss: 0.0577, Accuracy: 0.9582\n",
            "Training Accuracy for epoch 2: 0.9584\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0675, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1059, Accuracy: 0.9688\n",
            "Step 200, Loss: 0.0340, Accuracy: 0.9694\n",
            "Step 300, Loss: 0.3083, Accuracy: 0.9664\n",
            "Step 400, Loss: 0.0502, Accuracy: 0.9672\n",
            "Step 500, Loss: 0.1049, Accuracy: 0.9672\n",
            "Step 600, Loss: 0.2322, Accuracy: 0.9673\n",
            "Step 700, Loss: 0.0632, Accuracy: 0.9668\n",
            "Step 800, Loss: 0.0905, Accuracy: 0.9674\n",
            "Step 900, Loss: 0.0494, Accuracy: 0.9669\n",
            "Step 1000, Loss: 0.0658, Accuracy: 0.9673\n",
            "Step 1100, Loss: 0.0196, Accuracy: 0.9674\n",
            "Step 1200, Loss: 0.0290, Accuracy: 0.9678\n",
            "Step 1300, Loss: 0.0204, Accuracy: 0.9679\n",
            "Step 1400, Loss: 0.1091, Accuracy: 0.9679\n",
            "Step 1500, Loss: 0.0957, Accuracy: 0.9680\n",
            "Step 1600, Loss: 0.0419, Accuracy: 0.9684\n",
            "Step 1700, Loss: 0.0364, Accuracy: 0.9689\n",
            "Step 1800, Loss: 0.2240, Accuracy: 0.9688\n",
            "Training Accuracy for epoch 3: 0.9690\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.1152, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0177, Accuracy: 0.9780\n",
            "Step 200, Loss: 0.0149, Accuracy: 0.9754\n",
            "Step 300, Loss: 0.1289, Accuracy: 0.9738\n",
            "Step 400, Loss: 0.0294, Accuracy: 0.9733\n",
            "Step 500, Loss: 0.1561, Accuracy: 0.9736\n",
            "Step 600, Loss: 0.0301, Accuracy: 0.9738\n",
            "Step 700, Loss: 0.1181, Accuracy: 0.9741\n",
            "Step 800, Loss: 0.0323, Accuracy: 0.9739\n",
            "Step 900, Loss: 0.0158, Accuracy: 0.9742\n",
            "Step 1000, Loss: 0.0518, Accuracy: 0.9742\n",
            "Step 1100, Loss: 0.0105, Accuracy: 0.9743\n",
            "Step 1200, Loss: 0.0170, Accuracy: 0.9741\n",
            "Step 1300, Loss: 0.1494, Accuracy: 0.9738\n",
            "Step 1400, Loss: 0.0341, Accuracy: 0.9742\n",
            "Step 1500, Loss: 0.0550, Accuracy: 0.9743\n",
            "Step 1600, Loss: 0.0221, Accuracy: 0.9744\n",
            "Step 1700, Loss: 0.0166, Accuracy: 0.9745\n",
            "Step 1800, Loss: 0.0296, Accuracy: 0.9747\n",
            "Training Accuracy for epoch 4: 0.9748\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.1881, Accuracy: 0.9062\n",
            "Step 100, Loss: 0.0726, Accuracy: 0.9802\n",
            "Step 200, Loss: 0.0282, Accuracy: 0.9803\n",
            "Step 300, Loss: 0.0187, Accuracy: 0.9806\n",
            "Step 400, Loss: 0.1050, Accuracy: 0.9797\n",
            "Step 500, Loss: 0.0313, Accuracy: 0.9802\n",
            "Step 600, Loss: 0.0097, Accuracy: 0.9801\n",
            "Step 700, Loss: 0.0752, Accuracy: 0.9799\n",
            "Step 800, Loss: 0.0157, Accuracy: 0.9801\n",
            "Step 900, Loss: 0.0427, Accuracy: 0.9800\n",
            "Step 1000, Loss: 0.0760, Accuracy: 0.9796\n",
            "Step 1100, Loss: 0.0157, Accuracy: 0.9795\n",
            "Step 1200, Loss: 0.1177, Accuracy: 0.9798\n",
            "Step 1300, Loss: 0.0558, Accuracy: 0.9796\n",
            "Step 1400, Loss: 0.0366, Accuracy: 0.9793\n",
            "Step 1500, Loss: 0.1276, Accuracy: 0.9792\n",
            "Step 1600, Loss: 0.1949, Accuracy: 0.9793\n",
            "Step 1700, Loss: 0.0114, Accuracy: 0.9793\n",
            "Step 1800, Loss: 0.0663, Accuracy: 0.9796\n",
            "Training Accuracy for epoch 5: 0.9796\n",
            "\n",
            "TF Training time: 221.16 seconds\n",
            "Test Accuracy: 0.9723\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalization factor\n",
        "x_test = x_test / 255.0   # Normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Batch size as in first TF example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Defining model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Number of neurons and activation\n",
        "])\n",
        "\n",
        "# Defining loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E4Nlg4lb_qdW",
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jmu_hciK_qle",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmu_hciK_qle",
        "outputId": "e3a9b487-4b64-48ef-e2a4-e45f59ccd703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3811, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.5243, Accuracy: 0.7410\n",
            "Step 200, Loss: 0.3478, Accuracy: 0.8078\n",
            "Step 300, Loss: 0.2486, Accuracy: 0.8394\n",
            "Step 400, Loss: 0.5382, Accuracy: 0.8538\n",
            "Step 500, Loss: 0.3831, Accuracy: 0.8656\n",
            "Step 600, Loss: 0.4643, Accuracy: 0.8754\n",
            "Step 700, Loss: 0.2827, Accuracy: 0.8825\n",
            "Step 800, Loss: 0.0854, Accuracy: 0.8873\n",
            "Step 900, Loss: 0.4231, Accuracy: 0.8919\n",
            "Step 1000, Loss: 0.4092, Accuracy: 0.8955\n",
            "Step 1100, Loss: 0.1223, Accuracy: 0.8983\n",
            "Step 1200, Loss: 0.4102, Accuracy: 0.9006\n",
            "Step 1300, Loss: 0.2002, Accuracy: 0.9036\n",
            "Step 1400, Loss: 0.1474, Accuracy: 0.9059\n",
            "Step 1500, Loss: 0.0829, Accuracy: 0.9082\n",
            "Step 1600, Loss: 0.2678, Accuracy: 0.9105\n",
            "Step 1700, Loss: 0.2113, Accuracy: 0.9124\n",
            "Step 1800, Loss: 0.3125, Accuracy: 0.9141\n",
            "Training Accuracy for epoch 1: 0.9151\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1990, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1340, Accuracy: 0.9502\n",
            "Step 200, Loss: 0.3113, Accuracy: 0.9485\n",
            "Step 300, Loss: 0.3668, Accuracy: 0.9508\n",
            "Step 400, Loss: 0.0543, Accuracy: 0.9518\n",
            "Step 500, Loss: 0.3038, Accuracy: 0.9521\n",
            "Step 600, Loss: 0.1168, Accuracy: 0.9530\n",
            "Step 700, Loss: 0.1375, Accuracy: 0.9526\n",
            "Step 800, Loss: 0.2015, Accuracy: 0.9533\n",
            "Step 900, Loss: 0.0633, Accuracy: 0.9538\n",
            "Step 1000, Loss: 0.3608, Accuracy: 0.9538\n",
            "Step 1100, Loss: 0.1474, Accuracy: 0.9539\n",
            "Step 1200, Loss: 0.1257, Accuracy: 0.9544\n",
            "Step 1300, Loss: 0.0563, Accuracy: 0.9548\n",
            "Step 1400, Loss: 0.2095, Accuracy: 0.9553\n",
            "Step 1500, Loss: 0.0860, Accuracy: 0.9558\n",
            "Step 1600, Loss: 0.1298, Accuracy: 0.9563\n",
            "Step 1700, Loss: 0.1322, Accuracy: 0.9567\n",
            "Step 1800, Loss: 0.0242, Accuracy: 0.9575\n",
            "Training Accuracy for epoch 2: 0.9579\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0916, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.2280, Accuracy: 0.9688\n",
            "Step 200, Loss: 0.0572, Accuracy: 0.9652\n",
            "Step 300, Loss: 0.0409, Accuracy: 0.9661\n",
            "Step 400, Loss: 0.1855, Accuracy: 0.9671\n",
            "Step 500, Loss: 0.2360, Accuracy: 0.9674\n",
            "Step 600, Loss: 0.0682, Accuracy: 0.9674\n",
            "Step 700, Loss: 0.0744, Accuracy: 0.9680\n",
            "Step 800, Loss: 0.0830, Accuracy: 0.9682\n",
            "Step 900, Loss: 0.0162, Accuracy: 0.9685\n",
            "Step 1000, Loss: 0.0369, Accuracy: 0.9687\n",
            "Step 1100, Loss: 0.2381, Accuracy: 0.9688\n",
            "Step 1200, Loss: 0.0839, Accuracy: 0.9685\n",
            "Step 1300, Loss: 0.0666, Accuracy: 0.9681\n",
            "Step 1400, Loss: 0.0707, Accuracy: 0.9680\n",
            "Step 1500, Loss: 0.0738, Accuracy: 0.9682\n",
            "Step 1600, Loss: 0.0381, Accuracy: 0.9689\n",
            "Step 1700, Loss: 0.0259, Accuracy: 0.9693\n",
            "Step 1800, Loss: 0.0157, Accuracy: 0.9695\n",
            "Training Accuracy for epoch 3: 0.9695\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0788, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0389, Accuracy: 0.9722\n",
            "Step 200, Loss: 0.0305, Accuracy: 0.9729\n",
            "Step 300, Loss: 0.0625, Accuracy: 0.9727\n",
            "Step 400, Loss: 0.1710, Accuracy: 0.9748\n",
            "Step 500, Loss: 0.0823, Accuracy: 0.9742\n",
            "Step 600, Loss: 0.0286, Accuracy: 0.9735\n",
            "Step 700, Loss: 0.0331, Accuracy: 0.9736\n",
            "Step 800, Loss: 0.0617, Accuracy: 0.9738\n",
            "Step 900, Loss: 0.1046, Accuracy: 0.9739\n",
            "Step 1000, Loss: 0.0279, Accuracy: 0.9744\n",
            "Step 1100, Loss: 0.0139, Accuracy: 0.9743\n",
            "Step 1200, Loss: 0.0101, Accuracy: 0.9744\n",
            "Step 1300, Loss: 0.1894, Accuracy: 0.9742\n",
            "Step 1400, Loss: 0.1139, Accuracy: 0.9743\n",
            "Step 1500, Loss: 0.0803, Accuracy: 0.9740\n",
            "Step 1600, Loss: 0.0858, Accuracy: 0.9743\n",
            "Step 1700, Loss: 0.0081, Accuracy: 0.9745\n",
            "Step 1800, Loss: 0.0668, Accuracy: 0.9747\n",
            "Training Accuracy for epoch 4: 0.9751\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.1150, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0033, Accuracy: 0.9787\n",
            "Step 200, Loss: 0.1709, Accuracy: 0.9784\n",
            "Step 300, Loss: 0.0180, Accuracy: 0.9792\n",
            "Step 400, Loss: 0.0748, Accuracy: 0.9807\n",
            "Step 500, Loss: 0.1200, Accuracy: 0.9797\n",
            "Step 600, Loss: 0.2315, Accuracy: 0.9795\n",
            "Step 700, Loss: 0.0312, Accuracy: 0.9793\n",
            "Step 800, Loss: 0.0979, Accuracy: 0.9799\n",
            "Step 900, Loss: 0.0582, Accuracy: 0.9800\n",
            "Step 1000, Loss: 0.0132, Accuracy: 0.9801\n",
            "Step 1100, Loss: 0.2950, Accuracy: 0.9800\n",
            "Step 1200, Loss: 0.0797, Accuracy: 0.9801\n",
            "Step 1300, Loss: 0.0357, Accuracy: 0.9794\n",
            "Step 1400, Loss: 0.0104, Accuracy: 0.9795\n",
            "Step 1500, Loss: 0.1052, Accuracy: 0.9793\n",
            "Step 1600, Loss: 0.1279, Accuracy: 0.9797\n",
            "Step 1700, Loss: 0.1139, Accuracy: 0.9798\n",
            "Step 1800, Loss: 0.0494, Accuracy: 0.9795\n",
            "Training Accuracy for epoch 5: 0.9794\n",
            "\n",
            "TF Training time: 13.67 seconds\n",
            "Test Accuracy: 0.9716\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalization factor\n",
        "x_test = x_test / 255.0   # Normalization factor\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Defining model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    #  Number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Number of neurons and activation\n",
        "])\n",
        "\n",
        "# Defining loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compiling the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
