{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinithaRao22/tf-vs-pytorch-lab/blob/main/Lab03_TensorFlow_vs_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23ebc05e",
      "metadata": {
        "id": "23ebc05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "708a2cb9-1f25-413b-fb89-dcd961ac66ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.8618 - loss: 0.4941\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1545\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9687 - loss: 0.1087\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9759 - loss: 0.0809\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0677\n",
            "TF Training time: 33.46 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9661 - loss: 0.1088\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.09457345306873322, 0.9710999727249146]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255   # Fill in normalization factor\n",
        "x_test = x_test / 255     # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),        # Fill input shape\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Fill number of hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Fill number of output neurons\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # Fill name of loss function\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")       # Output training time\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be6ab50a",
      "metadata": {
        "id": "be6ab50a",
        "outputId": "279f8d71-b8f0-4435-b2a5-71912d8914be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp527a9i4f'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134940173694160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134940173696656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134940173696848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134940173695888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "623dfb49",
      "metadata": {
        "id": "623dfb49",
        "outputId": "dd205a8b-9d5a-4c6f-df4a-8433e5ed3be1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 64.67 seconds\n",
            "Test accuracy: 0.9633\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)    # Fill correct input and output size\n",
        "        self.fc2 = nn.Linear(64, 10)    # Fill correct input and output size\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))    # Fill correct layer\n",
        "        return self.fc2(x)    # Fill correct layer\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "WuMKMhHc8aLF",
        "outputId": "383b755f-19ba-4368-cbfe-a2847587f9df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WuMKMhHc8aLF",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ],
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "id": "sv4P-dSS_GQB"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Fill same batch size as in first TF example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "KH-sDlHq_Gdw",
        "outputId": "f7c8a661-fe09-4190-a703-0c110cb90c25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KH-sDlHq_Gdw",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.4497, Accuracy: 0.1250\n",
            "Step 100, Loss: 0.3338, Accuracy: 0.7160\n",
            "Step 200, Loss: 0.2329, Accuracy: 0.7956\n",
            "Step 300, Loss: 0.2287, Accuracy: 0.8275\n",
            "Step 400, Loss: 0.1954, Accuracy: 0.8476\n",
            "Step 500, Loss: 0.5050, Accuracy: 0.8595\n",
            "Step 600, Loss: 0.2571, Accuracy: 0.8697\n",
            "Step 700, Loss: 0.2180, Accuracy: 0.8762\n",
            "Step 800, Loss: 0.5224, Accuracy: 0.8820\n",
            "Step 900, Loss: 0.5155, Accuracy: 0.8870\n",
            "Step 1000, Loss: 0.1557, Accuracy: 0.8916\n",
            "Step 1100, Loss: 0.1322, Accuracy: 0.8961\n",
            "Step 1200, Loss: 0.2209, Accuracy: 0.8989\n",
            "Step 1300, Loss: 0.4585, Accuracy: 0.9021\n",
            "Step 1400, Loss: 0.1922, Accuracy: 0.9042\n",
            "Step 1500, Loss: 0.2942, Accuracy: 0.9068\n",
            "Step 1600, Loss: 0.2936, Accuracy: 0.9097\n",
            "Step 1700, Loss: 0.3227, Accuracy: 0.9119\n",
            "Step 1800, Loss: 0.0912, Accuracy: 0.9140\n",
            "Training Accuracy for epoch 1: 0.9153\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1152, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1537, Accuracy: 0.9502\n",
            "Step 200, Loss: 0.3175, Accuracy: 0.9524\n",
            "Step 300, Loss: 0.1929, Accuracy: 0.9514\n",
            "Step 400, Loss: 0.0916, Accuracy: 0.9519\n",
            "Step 500, Loss: 0.1572, Accuracy: 0.9528\n",
            "Step 600, Loss: 0.1983, Accuracy: 0.9532\n",
            "Step 700, Loss: 0.1215, Accuracy: 0.9535\n",
            "Step 800, Loss: 0.3792, Accuracy: 0.9539\n",
            "Step 900, Loss: 0.0254, Accuracy: 0.9544\n",
            "Step 1000, Loss: 0.1644, Accuracy: 0.9544\n",
            "Step 1100, Loss: 0.1216, Accuracy: 0.9550\n",
            "Step 1200, Loss: 0.1641, Accuracy: 0.9551\n",
            "Step 1300, Loss: 0.0640, Accuracy: 0.9557\n",
            "Step 1400, Loss: 0.0490, Accuracy: 0.9557\n",
            "Step 1500, Loss: 0.1023, Accuracy: 0.9565\n",
            "Step 1600, Loss: 0.0967, Accuracy: 0.9570\n",
            "Step 1700, Loss: 0.2672, Accuracy: 0.9578\n",
            "Step 1800, Loss: 0.2247, Accuracy: 0.9583\n",
            "Training Accuracy for epoch 2: 0.9585\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0717, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.2206, Accuracy: 0.9641\n",
            "Step 200, Loss: 0.0658, Accuracy: 0.9683\n",
            "Step 300, Loss: 0.0627, Accuracy: 0.9665\n",
            "Step 400, Loss: 0.0441, Accuracy: 0.9670\n",
            "Step 500, Loss: 0.0230, Accuracy: 0.9665\n",
            "Step 600, Loss: 0.0217, Accuracy: 0.9668\n",
            "Step 700, Loss: 0.1455, Accuracy: 0.9675\n",
            "Step 800, Loss: 0.1743, Accuracy: 0.9678\n",
            "Step 900, Loss: 0.0182, Accuracy: 0.9681\n",
            "Step 1000, Loss: 0.0192, Accuracy: 0.9683\n",
            "Step 1100, Loss: 0.0521, Accuracy: 0.9680\n",
            "Step 1200, Loss: 0.1945, Accuracy: 0.9683\n",
            "Step 1300, Loss: 0.0662, Accuracy: 0.9680\n",
            "Step 1400, Loss: 0.0481, Accuracy: 0.9680\n",
            "Step 1500, Loss: 0.0425, Accuracy: 0.9683\n",
            "Step 1600, Loss: 0.1806, Accuracy: 0.9685\n",
            "Step 1700, Loss: 0.1491, Accuracy: 0.9690\n",
            "Step 1800, Loss: 0.1577, Accuracy: 0.9693\n",
            "Training Accuracy for epoch 3: 0.9691\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0377, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0661, Accuracy: 0.9743\n",
            "Step 200, Loss: 0.0162, Accuracy: 0.9757\n",
            "Step 300, Loss: 0.0283, Accuracy: 0.9752\n",
            "Step 400, Loss: 0.0988, Accuracy: 0.9747\n",
            "Step 500, Loss: 0.0260, Accuracy: 0.9746\n",
            "Step 600, Loss: 0.0332, Accuracy: 0.9745\n",
            "Step 700, Loss: 0.0940, Accuracy: 0.9753\n",
            "Step 800, Loss: 0.0064, Accuracy: 0.9754\n",
            "Step 900, Loss: 0.0983, Accuracy: 0.9754\n",
            "Step 1000, Loss: 0.0210, Accuracy: 0.9752\n",
            "Step 1100, Loss: 0.2364, Accuracy: 0.9750\n",
            "Step 1200, Loss: 0.0425, Accuracy: 0.9751\n",
            "Step 1300, Loss: 0.1539, Accuracy: 0.9750\n",
            "Step 1400, Loss: 0.0467, Accuracy: 0.9749\n",
            "Step 1500, Loss: 0.0902, Accuracy: 0.9748\n",
            "Step 1600, Loss: 0.0224, Accuracy: 0.9748\n",
            "Step 1700, Loss: 0.0061, Accuracy: 0.9751\n",
            "Step 1800, Loss: 0.1931, Accuracy: 0.9752\n",
            "Training Accuracy for epoch 4: 0.9753\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0864, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0045, Accuracy: 0.9799\n",
            "Step 200, Loss: 0.0306, Accuracy: 0.9795\n",
            "Step 300, Loss: 0.0433, Accuracy: 0.9788\n",
            "Step 400, Loss: 0.0295, Accuracy: 0.9791\n",
            "Step 500, Loss: 0.3707, Accuracy: 0.9794\n",
            "Step 600, Loss: 0.0124, Accuracy: 0.9796\n",
            "Step 700, Loss: 0.0086, Accuracy: 0.9790\n",
            "Step 800, Loss: 0.0499, Accuracy: 0.9788\n",
            "Step 900, Loss: 0.0880, Accuracy: 0.9791\n",
            "Step 1000, Loss: 0.1918, Accuracy: 0.9792\n",
            "Step 1100, Loss: 0.0426, Accuracy: 0.9790\n",
            "Step 1200, Loss: 0.0976, Accuracy: 0.9789\n",
            "Step 1300, Loss: 0.0682, Accuracy: 0.9784\n",
            "Step 1400, Loss: 0.0719, Accuracy: 0.9783\n",
            "Step 1500, Loss: 0.0351, Accuracy: 0.9784\n",
            "Step 1600, Loss: 0.0203, Accuracy: 0.9785\n",
            "Step 1700, Loss: 0.0149, Accuracy: 0.9787\n",
            "Step 1800, Loss: 0.0076, Accuracy: 0.9788\n",
            "Training Accuracy for epoch 5: 0.9789\n",
            "\n",
            "TF Training time: 385.57 seconds\n",
            "Test Accuracy: 0.9744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ],
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "id": "E4Nlg4lb_qdW"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "Jmu_hciK_qle",
        "outputId": "9a055358-d4a7-4f42-d234-7ce7a552f425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Jmu_hciK_qle",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3801, Accuracy: 0.1250\n",
            "Step 100, Loss: 0.5791, Accuracy: 0.7287\n",
            "Step 200, Loss: 0.6438, Accuracy: 0.8004\n",
            "Step 300, Loss: 0.5098, Accuracy: 0.8339\n",
            "Step 400, Loss: 0.6336, Accuracy: 0.8527\n",
            "Step 500, Loss: 0.2519, Accuracy: 0.8640\n",
            "Step 600, Loss: 0.5093, Accuracy: 0.8739\n",
            "Step 700, Loss: 0.1533, Accuracy: 0.8817\n",
            "Step 800, Loss: 0.1979, Accuracy: 0.8867\n",
            "Step 900, Loss: 0.6330, Accuracy: 0.8909\n",
            "Step 1000, Loss: 0.2887, Accuracy: 0.8946\n",
            "Step 1100, Loss: 0.4244, Accuracy: 0.8983\n",
            "Step 1200, Loss: 0.2061, Accuracy: 0.9010\n",
            "Step 1300, Loss: 0.0990, Accuracy: 0.9028\n",
            "Step 1400, Loss: 0.3325, Accuracy: 0.9052\n",
            "Step 1500, Loss: 0.1648, Accuracy: 0.9076\n",
            "Step 1600, Loss: 0.2779, Accuracy: 0.9096\n",
            "Step 1700, Loss: 0.1347, Accuracy: 0.9120\n",
            "Step 1800, Loss: 0.1129, Accuracy: 0.9143\n",
            "Training Accuracy for epoch 1: 0.9160\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.2344, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0783, Accuracy: 0.9530\n",
            "Step 200, Loss: 0.4860, Accuracy: 0.9548\n",
            "Step 300, Loss: 0.0547, Accuracy: 0.9551\n",
            "Step 400, Loss: 0.2246, Accuracy: 0.9556\n",
            "Step 500, Loss: 0.1603, Accuracy: 0.9559\n",
            "Step 600, Loss: 0.1372, Accuracy: 0.9558\n",
            "Step 700, Loss: 0.0283, Accuracy: 0.9560\n",
            "Step 800, Loss: 0.0627, Accuracy: 0.9560\n",
            "Step 900, Loss: 0.0643, Accuracy: 0.9558\n",
            "Step 1000, Loss: 0.1557, Accuracy: 0.9565\n",
            "Step 1100, Loss: 0.0232, Accuracy: 0.9563\n",
            "Step 1200, Loss: 0.0505, Accuracy: 0.9557\n",
            "Step 1300, Loss: 0.0632, Accuracy: 0.9566\n",
            "Step 1400, Loss: 0.1244, Accuracy: 0.9566\n",
            "Step 1500, Loss: 0.2116, Accuracy: 0.9566\n",
            "Step 1600, Loss: 0.2185, Accuracy: 0.9570\n",
            "Step 1700, Loss: 0.2394, Accuracy: 0.9578\n",
            "Step 1800, Loss: 0.2757, Accuracy: 0.9580\n",
            "Training Accuracy for epoch 2: 0.9582\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1246, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0388, Accuracy: 0.9669\n",
            "Step 200, Loss: 0.0592, Accuracy: 0.9658\n",
            "Step 300, Loss: 0.0725, Accuracy: 0.9665\n",
            "Step 400, Loss: 0.1706, Accuracy: 0.9671\n",
            "Step 500, Loss: 0.0299, Accuracy: 0.9676\n",
            "Step 600, Loss: 0.2010, Accuracy: 0.9681\n",
            "Step 700, Loss: 0.0820, Accuracy: 0.9685\n",
            "Step 800, Loss: 0.2911, Accuracy: 0.9686\n",
            "Step 900, Loss: 0.0318, Accuracy: 0.9688\n",
            "Step 1000, Loss: 0.1354, Accuracy: 0.9690\n",
            "Step 1100, Loss: 0.0828, Accuracy: 0.9693\n",
            "Step 1200, Loss: 0.4240, Accuracy: 0.9693\n",
            "Step 1300, Loss: 0.0709, Accuracy: 0.9689\n",
            "Step 1400, Loss: 0.0807, Accuracy: 0.9688\n",
            "Step 1500, Loss: 0.1277, Accuracy: 0.9687\n",
            "Step 1600, Loss: 0.0174, Accuracy: 0.9691\n",
            "Step 1700, Loss: 0.0192, Accuracy: 0.9693\n",
            "Step 1800, Loss: 0.0595, Accuracy: 0.9697\n",
            "Training Accuracy for epoch 3: 0.9699\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0413, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0582, Accuracy: 0.9765\n",
            "Step 200, Loss: 0.1591, Accuracy: 0.9767\n",
            "Step 300, Loss: 0.1035, Accuracy: 0.9759\n",
            "Step 400, Loss: 0.0820, Accuracy: 0.9750\n",
            "Step 500, Loss: 0.0697, Accuracy: 0.9751\n",
            "Step 600, Loss: 0.0397, Accuracy: 0.9750\n",
            "Step 700, Loss: 0.1036, Accuracy: 0.9749\n",
            "Step 800, Loss: 0.0371, Accuracy: 0.9748\n",
            "Step 900, Loss: 0.0300, Accuracy: 0.9746\n",
            "Step 1000, Loss: 0.2929, Accuracy: 0.9751\n",
            "Step 1100, Loss: 0.1596, Accuracy: 0.9752\n",
            "Step 1200, Loss: 0.0481, Accuracy: 0.9752\n",
            "Step 1300, Loss: 0.0845, Accuracy: 0.9749\n",
            "Step 1400, Loss: 0.0261, Accuracy: 0.9749\n",
            "Step 1500, Loss: 0.0093, Accuracy: 0.9750\n",
            "Step 1600, Loss: 0.0281, Accuracy: 0.9752\n",
            "Step 1700, Loss: 0.1218, Accuracy: 0.9755\n",
            "Step 1800, Loss: 0.0375, Accuracy: 0.9759\n",
            "Training Accuracy for epoch 4: 0.9759\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.1100, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0498, Accuracy: 0.9827\n",
            "Step 200, Loss: 0.0319, Accuracy: 0.9813\n",
            "Step 300, Loss: 0.2154, Accuracy: 0.9804\n",
            "Step 400, Loss: 0.1935, Accuracy: 0.9800\n",
            "Step 500, Loss: 0.1856, Accuracy: 0.9800\n",
            "Step 600, Loss: 0.2056, Accuracy: 0.9789\n",
            "Step 700, Loss: 0.1580, Accuracy: 0.9794\n",
            "Step 800, Loss: 0.2993, Accuracy: 0.9792\n",
            "Step 900, Loss: 0.0441, Accuracy: 0.9792\n",
            "Step 1000, Loss: 0.0045, Accuracy: 0.9794\n",
            "Step 1100, Loss: 0.0779, Accuracy: 0.9795\n",
            "Step 1200, Loss: 0.0885, Accuracy: 0.9798\n",
            "Step 1300, Loss: 0.0863, Accuracy: 0.9798\n",
            "Step 1400, Loss: 0.0141, Accuracy: 0.9799\n",
            "Step 1500, Loss: 0.1849, Accuracy: 0.9798\n",
            "Step 1600, Loss: 0.0679, Accuracy: 0.9801\n",
            "Step 1700, Loss: 0.0463, Accuracy: 0.9799\n",
            "Step 1800, Loss: 0.0358, Accuracy: 0.9802\n",
            "Training Accuracy for epoch 5: 0.9801\n",
            "\n",
            "TF Training time: 23.06 seconds\n",
            "Test Accuracy: 0.9739\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}