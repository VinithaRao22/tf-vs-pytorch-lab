{"cells":[{"cell_type":"markdown","id":"5c1e84a1","metadata":{"id":"5c1e84a1"},"source":["# Lab 03: TensorFlow vs. PyTorch\n","- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n","- Use tf.GradientTape for Tensorflow custom training loop.\n","\n"]},{"cell_type":"markdown","id":"a1c58bba","metadata":{"id":"a1c58bba"},"source":["## TensorFlow Implementation"]},{"cell_type":"code","execution_count":4,"id":"23ebc05e","metadata":{"id":"23ebc05e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744037129885,"user_tz":-120,"elapsed":27541,"user":{"displayName":"Tobias Schaffer","userId":"09009542802022320095"}},"outputId":"24c0a299-6c6e-44b4-bddc-08794750b2a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8583 - loss: 0.4880\n","Epoch 2/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9618 - loss: 0.1268\n","Epoch 3/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.0908\n","Epoch 4/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.0726\n","Epoch 5/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9823 - loss: 0.0581\n","TF Training time: 26.16 seconds\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9710 - loss: 0.0939\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.08851958066225052, 0.9732000231742859]"]},"metadata":{},"execution_count":4}],"source":["import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import time\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train / 255   # Fill in normalization factor\n","x_test = x_test / 255     # Fill in normalization factor\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(28, 28)),        # Fill input shape\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(64, activation='relu'),  # Fill number of hidden neurons\n","    tf.keras.layers.Dense(10, activation='softmax')  # Fill number of output neurons\n","])\n","\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',       # Fill name of loss function\n","              metrics=['accuracy'])\n","\n","start = time.time()\n","model.fit(x_train, y_train, epochs=5)\n","end = time.time()\n","print(f\"TF Training time: {end-start:.2f} seconds\")       # Output training time\n","model.evaluate(x_test, y_test)"]},{"cell_type":"markdown","id":"72743ab8","metadata":{"id":"72743ab8"},"source":["## Convert TensorFlow model to TFLite"]},{"cell_type":"code","execution_count":null,"id":"be6ab50a","metadata":{"id":"be6ab50a"},"outputs":[],"source":["converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","with open(\"model.tflite\", \"wb\") as f:\n","    f.write(tflite_model)"]},{"cell_type":"markdown","id":"57c00c95","metadata":{"id":"57c00c95"},"source":["## PyTorch Implementation"]},{"cell_type":"code","execution_count":null,"id":"623dfb49","metadata":{"id":"623dfb49"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n","train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n","test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(___, ___)    # Fill correct input and output size\n","        self.fc2 = nn.Linear(___, ___)    # Fill correct input and output size\n","    def forward(self, x):\n","        x = F.relu(self.___(x))    # Fill correct layer\n","        return self.___(x)    # Fill correct layer\n","\n","model = Net()\n","optimizer = optim.Adam(model.parameters())\n","loss_fn = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","for epoch in range(5):\n","    for x, y in train_loader:\n","        optimizer.zero_grad()\n","        pred = model(x)\n","        loss = loss_fn(pred, y)\n","        loss.backward()\n","        optimizer.step()\n","end = time.time()\n","print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n","\n","model.eval()\n","correct = 0\n","with torch.no_grad():\n","    for x, y in test_loader:\n","        output = model(x)\n","        pred = output.argmax(1)\n","        correct += (pred == y).sum().item()\n","print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")"]},{"cell_type":"markdown","id":"f6dbdab0","metadata":{"id":"f6dbdab0"},"source":["## Convert PyTorch model to ONNX"]},{"cell_type":"code","source":["# Install ONNX\n","!pip install onnx"],"metadata":{"id":"WuMKMhHc8aLF"},"id":"WuMKMhHc8aLF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"09925e9a","metadata":{"id":"09925e9a"},"outputs":[],"source":["dummy_input = torch.randn(1, 784)\n","torch.onnx.export(model, dummy_input, \"model.onnx\",\n","                  input_names=[\"input\"], output_names=[\"output\"])"]},{"cell_type":"markdown","source":["## TensorFlow custom training loop using tf.GradientTape"],"metadata":{"id":"sv4P-dSS_GQB"},"id":"sv4P-dSS_GQB"},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import time\n","\n","# Load and preprocess data\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train / ___   # Fill in normalization factor\n","x_test = x_test / ___   # Fill in normalization factor\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","# Prepare datasets\n","batch_size = __         # Fill same batch size as in first TF example\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n","test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n","\n","# Define model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(___, ___)),    # Fill size\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(___, activation='___'),    # Fill number of neurons and activation\n","    tf.keras.layers.Dense(___, activation='___')     # Fill number of neurons and activation\n","])\n","\n","# Define loss, optimizer, and metrics\n","loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","\n","# Training loop\n","epochs = 5\n","start = time.time()\n","for epoch in range(epochs):\n","    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n","    for step, (x_batch, y_batch) in enumerate(train_dataset):\n","        with tf.GradientTape() as tape:\n","            logits = model(x_batch, training=True)\n","            loss = loss_fn(y_batch, logits)\n","        grads = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","        train_acc_metric.update_state(y_batch, logits)\n","\n","        if step % 100 == 0:\n","            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n","\n","    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n","    train_acc_metric.reset_state()\n","end = time.time()\n","print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n","\n","# Evaluation loop\n","for x_batch, y_batch in test_dataset:\n","    test_logits = model(x_batch, training=False)\n","    test_acc_metric.update_state(y_batch, test_logits)\n","\n","print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"],"metadata":{"id":"KH-sDlHq_Gdw"},"id":"KH-sDlHq_Gdw","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Performance Otimization with Graph Execution using @tf.function"],"metadata":{"id":"E4Nlg4lb_qdW"},"id":"E4Nlg4lb_qdW"},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import time\n","\n","# Load and preprocess data\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train / ___   # Fill in normalization factor\n","x_test = x_test / ___   # Fill in normalization factor\n","y_train = to_categorical(y_train, num_classes=10)\n","y_test = to_categorical(y_test, num_classes=10)\n","\n","# Prepare datasets\n","batch_size = 32\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n","test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n","\n","# Define model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(___, ___)),    # Fill size\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(___, activation='___'),    # Fill number of neurons and activation\n","    tf.keras.layers.Dense(___, activation='___')     # Fill number of neurons and activation\n","])\n","\n","# Define loss, optimizer, and metrics\n","loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","\n","@tf.function  # compile the function into a graph\n","def train_step(x_batch, y_batch):\n","    with tf.GradientTape() as tape:\n","        logits = model(x_batch, training=True)\n","        loss = loss_fn(y_batch, logits)\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","    train_acc_metric.update_state(y_batch, logits)\n","    return loss\n","\n","# Training loop\n","epochs = 5\n","start = time.time()\n","for epoch in range(epochs):\n","    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n","    for step, (x_batch, y_batch) in enumerate(train_dataset):\n","        loss = train_step(x_batch, y_batch)\n","\n","        if step % 100 == 0:\n","            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n","\n","    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n","    train_acc_metric.reset_state()\n","end = time.time()\n","print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n","\n","# Evaluation loop\n","for x_batch, y_batch in test_dataset:\n","    test_logits = model(x_batch, training=False)\n","    test_acc_metric.update_state(y_batch, test_logits)\n","\n","print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"],"metadata":{"id":"Jmu_hciK_qle"},"id":"Jmu_hciK_qle","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}